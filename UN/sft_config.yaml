# SFT Training Configuration
# Optimized for Qwen2.5-1.5B

# Model configuration
model_name_or_path: "Qwen/Qwen2.5-1.5B"
use_flash_attention: false
max_seq_length: 512  # Reduced to 512 for 11GB GPU compatibility, keeps ~63% of data

# LoRA configuration
use_lora: true
lora_r: 32  # Reduced for smaller model
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: "q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj"

# Quantization (set one to true if needed for memory)
use_4bit: true  # Enable 4-bit quantization to reduce memory usage
use_8bit: false

# Data configuration
dataset_path: "./data/processed/sft"
max_train_samples: 10000  # Limit to 10k samples (set to null for full dataset)

# Output configuration
output_dir: "./models/sft_model"

# Training arguments (following HF TrainingArguments)
# Multi-GPU: These settings enable DDP automatically when multiple GPUs are available

# Batch sizes (absolute minimum for 11GB RTX 2080 Ti)
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16  # Effective batch size = 1 * 16 * 2 GPUs = 32

# Learning rate (slightly higher for smaller model)
learning_rate: 3.0e-5
lr_scheduler_type: "cosine"
warmup_ratio: 0.03

# Training steps
num_train_epochs: 3
max_steps: -1  # Set to specific number if you want to limit steps

# Optimization
optim: "adamw_torch"
weight_decay: 0.01
max_grad_norm: 1.0
gradient_checkpointing: true

# Mixed precision (RTX 2080 Ti uses fp16, not bf16)
bf16: false  # RTX 2080 Ti doesn't support bf16 well (need Ampere+)
fp16: true  # Use fp16 for Turing GPUs (RTX 2080 Ti)
tf32: false  # Disabled - GPU doesn't support TF32 (requires Ampere or newer)

# Evaluation
eval_strategy: "steps"
eval_steps: 100
save_strategy: "steps"
save_steps: 100
save_total_limit: 3
load_best_model_at_end: false  # Disabled to avoid OOM when loading best checkpoint
metric_for_best_model: "eval_loss"

# Logging
logging_steps: 10
logging_first_step: true
report_to: "none"  # Options: wandb, tensorboard, none (changed to none - no WandB API key)

# Distributed training (automatically handled by accelerate/torchrun)
ddp_find_unused_parameters: false
ddp_timeout: 3600

# Other
seed: 42
dataloader_num_workers: 4
remove_unused_columns: false
