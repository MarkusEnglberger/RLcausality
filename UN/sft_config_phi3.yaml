# SFT Training Configuration for Phi-3.5-Mini (3.8B parameters)
# Phi-3.5-Mini is optimized for reasoning tasks and instruction following

# Model configuration
model_name_or_path: "microsoft/Phi-3.5-mini-instruct"
use_flash_attention: false
max_seq_length: 512  # Phi-3.5 supports up to 128k, but 512 is sufficient for this task

# LoRA configuration
# Phi-3.5 has different attention architecture - target modules are different from Qwen
use_lora: true
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
# Phi-3.5 uses different module names: qkv_proj (combined), o_proj, gate_up_proj (combined), down_proj
lora_target_modules: "qkv_proj,o_proj,gate_up_proj,down_proj"

# Quantization (4-bit for memory efficiency)
use_4bit: true
bnb_4bit_compute_dtype: "float16"
bnb_4bit_quant_type: "nf4"

# Dataset configuration
dataset_path: "./data/processed/sft"
max_train_samples: 10000  # Start with 10k, increase to null for full 200k dataset later

# Output configuration
output_dir: "./models/phi3_sft_model"

# Training arguments (following HF TrainingArguments)
# Multi-GPU: These settings enable DDP automatically when multiple GPUs are available

# Batch sizes (optimized for 3.8B model with 4-bit quantization on L4 GPUs)
per_device_train_batch_size: 2  # Can use 2 instead of 1 with Phi-3.5's efficient architecture
per_device_eval_batch_size: 2
gradient_accumulation_steps: 8  # Effective batch size = 2 * 8 * 2 GPUs = 32

# Learning rate (Phi-3.5 works well with slightly lower LR for fine-tuning)
learning_rate: 2.0e-5
lr_scheduler_type: "cosine"
warmup_ratio: 0.03

# Training steps
num_train_epochs: 3
max_steps: -1  # Set to specific number if you want to limit steps

# Optimization
optim: "adamw_torch"
weight_decay: 0.01
max_grad_norm: 1.0
gradient_checkpointing: true

# Mixed precision (L4 GPUs support bf16, which is better for Phi-3.5)
bf16: true   # L4 GPUs have Ampere architecture - use bf16 for better numerical stability
fp16: false  # Don't use fp16 when bf16 is available
tf32: true   # Enable TF32 for faster matmul on Ampere+ GPUs

# Evaluation
eval_strategy: "steps"
eval_steps: 100
save_strategy: "steps"
save_steps: 100
save_total_limit: 3
load_best_model_at_end: false  # Disabled to avoid OOM when loading best checkpoint
metric_for_best_model: "eval_loss"

# Logging
logging_steps: 10
logging_first_step: true
report_to: "none"  # Options: wandb, tensorboard, none

# Distributed training (automatically handled by accelerate/torchrun)
ddp_find_unused_parameters: false
ddp_timeout: 3600

# Other
seed: 42
dataloader_num_workers: 4
remove_unused_columns: false

# Phi-3.5 specific notes:
# - Uses custom chat template (handled automatically by tokenizer)
# - Model is pre-trained on reasoning tasks, so may converge faster
# - Supports long context (128k) but we limit to 512 for this task
# - Expected SFT accuracy: 70-75% (vs Qwen's ~65%)
