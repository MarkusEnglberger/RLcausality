# GRPO Training Configuration
# Optimized for DeepSeek-R1-Distill-Qwen-7B (7B parameter model)
# DeepSeek-R1 is specifically trained for reasoning, making it ideal for GRPO

# Model configuration
#model_name_or_path: "./models/deepseek_sft_model"  # Path to DeepSeek SFT checkpoint
#model_name_or_path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
model_name_or_path: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
max_prompt_length: 1500
max_completion_length: 2000 # Reduced to save memory during generation


# 4-bit has dtype issues with DeepSpeed parameter gathering during generation
use_4bit: true
#use_8bit: true

# Data configuration
dataset_path: "./data/processed/grpo_deepseek"
max_train_samples: 10000  

# Output configuration
output_dir: "./models/deepseek_grpo_model7b"

# Sample logging configuration (using TRL's built-in logging)
log_completions: true  # TRL built-in feature to log prompts and completions
num_sample_generations: 3  # Number of completions to log per step

# GRPO-specific training arguments
# Multi-GPU: These settings enable DDP automatically

# Batch sizes (for DeepSpeed ZeRO-3)
per_device_train_batch_size: 4  # Use 1 with DeepSpeed for maximum memory savings
per_device_eval_batch_size: 12  # Must be 2 so global batch (2*2=4) is divisible by num_generations (4)
gradient_accumulation_steps: 1  # Increased to maintain effective batch size

# Learning rate (very low for GRPO on large model)
learning_rate:  1.0e-5 
lr_scheduler_type: "cosine"
warmup_ratio: 0.01

# Training steps
num_train_epochs: 1
max_steps: -1

# GRPO hyperparameters
num_iterations: 4
num_generations: 4  # Number of samples to generate per prompt
temperature: 0.6  # DeepSeek-R1 recommended temperature (0.5-0.7)
beta: 0.04  # KL divergence coefficient

# Optimization
optim: "adamw_torch"
weight_decay: 0.01
max_grad_norm: 1.0
gradient_checkpointing: true  # Essential for 7B model

# Mixed precision (depends on GPU generation)
bf16: true  # Set to true for Ampere+ GPUs (A100, RTX 3090+)
fp16: false  # Use fp16 for Turing GPUs (RTX 2080 Ti)
tf32: false  # Enable on Ampere+ GPUs

# Evaluation
eval_strategy: "steps"
eval_steps: 50
save_strategy: "steps"
save_steps: 20
save_total_limit: 3

# Logging
logging_steps: 10
logging_first_step: true
report_to: "none"  #"wandb"  # "none"

# Distributed training
ddp_find_unused_parameters: false
ddp_timeout: 3600

# Other
seed: 42
dataloader_num_workers: 8
remove_unused_columns: false

# DeepSeek-R1 specific notes:
# - Model is pre-trained with reasoning chains using <think> tags
# - Naturally suited for GRPO due to reasoning capabilities
# - Temperature 0.6 recommended (range: 0.5-0.7)
# - No system prompts needed - use user prompts only
# - Longer completions expected due to reasoning chains
