# GRPO Training Configuration for Phi-3.5-Mini (3.8B parameters)
# Optimized for reasoning task with causal inference

# Model configuration
# Using checkpoint-900 (96% of training) - effectively fully trained
model_name_or_path: "./models/phi3_sft_model/checkpoint-900"  # Path to SFT checkpoint
model_revision: "3145e03a9fd4cdd7cd953c34d9bbf7ad606122ca"  # Pin revision to prevent re-downloading incompatible code
use_flash_attention: false
max_seq_length: 512

# Phi-3.5 compatibility: Use eager attention to avoid DynamicCache issues
# The custom Phi-3 modeling code has compatibility issues with some transformers versions
attn_implementation: "eager"

# LoRA configuration (must match SFT training)
use_lora: true
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: "qkv_proj,o_proj,gate_up_proj,down_proj"

# Quantization
use_4bit: true
bnb_4bit_compute_dtype: "float16"
bnb_4bit_quant_type: "nf4"

# Dataset configuration
dataset_path: "./data/processed/grpo"
max_train_samples: 5000  # 5k samples for GRPO training

# Output configuration
output_dir: "./models/phi3_grpo_model"
run_name: "phi3-grpo-corr2cause"

# GRPO-specific parameters
# num_generations: Number of completions to generate per prompt for reward comparison
# Must be a divisor of generation_batch_size (default 16)
num_generations: 4  # Use 4 (divisor of 16) for compatibility

# generation_batch_size: Must be divisible by num_generations
# Default is 16, which works with num_generations=4
generation_batch_size: 16

# temperature: Sampling temperature for generation diversity
# Higher = more exploration, lower = more exploitation
temperature: 0.9  # Slightly higher than Qwen for better exploration

# beta: KL divergence coefficient (penalty for drifting from SFT policy)
# Lower = more exploration, higher = stay closer to SFT
beta: 0.03  # Lower than Qwen (0.05) - Phi-3.5 needs less constraint due to better reasoning

# Training arguments
per_device_train_batch_size: 4  # Can use larger batch with Phi-3.5's efficiency
gradient_accumulation_steps: 2  # Effective batch = 4 * 6 generations * 2 accum * 2 GPUs = high variance reduction
num_train_epochs: 1
max_steps: -1

# Learning rate (lower for GRPO stability)
learning_rate: 5.0e-7  # Slightly lower than Qwen (1e-6) for more stable convergence
lr_scheduler_type: "constant"  # Constant LR for RL is often better than decay
warmup_steps: 10

# Optimization
optim: "adamw_torch"
weight_decay: 0.0  # No weight decay for RL fine-tuning
max_grad_norm: 1.0
gradient_checkpointing: true

# Mixed precision
# Note: bf16/tf32 only work on Ampere+ GPUs (L4, A100, etc.)
# If you get older GPUs (RTX 2080 Ti, V100), these will auto-disable and fall back to fp16
bf16: false  # Disable bf16 for compatibility with older GPUs
fp16: true   # Use fp16 for broader GPU compatibility
tf32: false  # Disable tf32 - only works on Ampere+ architecture

# Evaluation and checkpointing
eval_strategy: "no"  # GRPO doesn't use validation set during training
save_strategy: "steps"
save_steps: 50  # More frequent saves to catch best checkpoint
save_total_limit: 5  # Keep more checkpoints for Phi-3.5 (may want to compare)
logging_steps: 1  # Log every step to monitor reward progression

# Distributed training
ddp_find_unused_parameters: false
ddp_timeout: 3600

# Other
seed: 42
dataloader_num_workers: 4
remove_unused_columns: false
report_to: "none"
